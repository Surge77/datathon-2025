# ğŸ“Š Skill Gap Analysis & Career Path Optimization

> A data-driven exploration of the global tech job market â€” identifying the most in-demand skills, analyzing salary trends, and revealing actionable insights to bridge the gap between industry demand and workforce readiness.

---

## ğŸ‘¥ Team Members

* **Tirth Chankeshwara**
* **Tejas Deshmane**
* **Mahesh Bhimde**

---

## ğŸ¯ Problem Statement

The rapid evolution of technology has created a significant **skill gap** between what industries demand and what professionals currently possess. This project aims to:

* Forecast the **future demand** of tech skills (12â€“18 months ahead)
* Quantify **skill gaps** across roles, locations, and experience levels
* Analyze **salary variations** based on skill combinations
* Provide **data-driven career recommendations** and **learning paths**
* Visualize skill trends and insights through an **interactive dashboard**

---

## ğŸ—‚ï¸ Data Sources

We integrated three primary Kaggle datasets:

1. **Data Science Job Market (brsahan)** â€“ 10K+ listings; job titles, locations, salaries, skills.
2. **Job-Skill-Set (batuhanmutlu)** â€“ curated mapping of job roles to extracted skill sets.
3. **Global AI Job Market (bismasajjad)** â€“ 15K+ listings across 50+ countries with salary, experience, and company data.

**Integration Strategy:** Columns were harmonized (job titles, skills, currencies, experience) and merged into a unified `cleaned_master.csv` dataset.

---

## âš™ï¸ Methodology Overview

### **Phase 1 â€“ Data Collection & Cleaning**

* Gathered raw datasets from Kaggle sources.
* Standardized column names and data types.
* Handled missing values, duplicates, and outliers.
* Generated `data_quality_report.json` for validation metrics.

### **Phase 2 â€“ Pre-processing & Feature Engineering**

* Standardized skills using text normalization and taxonomy mapping.
* Engineered new features: `salary_midpoint`, `skill_count`, `posting_year`, etc.
* Created profiling snapshots (categorical & numerical).
* Exported cleaned dataset (`cleaned_master.csv`) for downstream analysis.

### **Phase 3 â€“ Exploratory Data Analysis (Completed)**

* Conducted univariate, bivariate, and multivariate analyses on the cleaned master dataset.
* Identified correlations, temporal hiring trends, and salary distributions.
* Generated 30+ visualizations automatically saved under `outputs/figures/`.
* Derived actionable insights:

  * Salary increases significantly with seniority and full-time roles.
  * Remote jobs show competitive compensation levels.
  * Top in-demand skills: **Python, SQL, Machine Learning, Cloud, Tableau**.
  * Emerging hubs: **USA, India, UK, and Germany** dominate tech hiring.
* Produced structured outputs: `eda_summary.json`, `eda_kpis.json`, and multiple CSV summaries in `/outputs/tables/`.

### **Phase 4 â€“ Modeling & Forecasting (Upcoming)**

* Apply ML algorithms: Prophet/ARIMA for skill forecasting, XGBoost for salary prediction.
* Perform clustering to detect job role patterns and emerging skill sets.

### **Phase 5 â€“ Visualization & Reporting (Upcoming)**

* Develop a Power BI dashboard showing salary trends, skill forecasts, and market gaps.
* Submit final report and presentation as per Datathon guidelines.

---

## ğŸ“ Repository Structure

```
datathon-2025/
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ dashboards/                      # Power BI / Tableau dashboards
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ README.md                    # Data dictionary and sources
â”‚   â”œâ”€â”€ external/                    # Supporting or reference datasets
â”‚   â”œâ”€â”€ processed/                   # Cleaned and merged data
â”‚   â”‚   â””â”€â”€ cleaned_master.csv
â”‚   â””â”€â”€ raw/                         # Original Kaggle datasets
â”‚       â”œâ”€â”€ dataset1_data_science_job.csv
â”‚       â”œâ”€â”€ dataset2_all_job_post.csv
â”‚       â”œâ”€â”€ dataset3_ai_job_dataset.csv
â”‚       â””â”€â”€ dataset3_ai_job_dataset1.csv
â”‚
â”œâ”€â”€ models/                          # Trained model files (.pkl, .sav)
â”‚
â”œâ”€â”€ notebooks/                       # Jupyter notebooks for each phase
â”‚   â”œâ”€â”€ 01_data_collection.ipynb
â”‚   â”œâ”€â”€ 02_data_cleaning.ipynb
â”‚   â””â”€â”€ 03_eda.ipynb
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ figures/                     # Auto-saved EDA plots (univariate, temporal, etc.)
â”‚   â”œâ”€â”€ insights/                    # Textual insight summaries
â”‚   â””â”€â”€ tables/                      # Aggregated metrics / CSV summaries
â”‚
â”œâ”€â”€ reports/                         # Reports and profiling results
â”‚   â”œâ”€â”€ data_quality_report.json
â”‚   â”œâ”€â”€ eda_summary.json
â”‚   â”œâ”€â”€ eda_kpis.json
â”‚   â”œâ”€â”€ profiling_categorical_snapshot.csv
â”‚   â””â”€â”€ profiling_numeric_snapshot.csv
â”‚
â””â”€â”€ src/                             # Python scripts for preprocessing & modeling
```

---

## ğŸ§® Key Deliverables (Phase-Wise)

| Phase   | Deliverable                                        | Status      |
| ------- | -------------------------------------------------- | ----------- |
| Phase 1 | Dataset sourcing, schema validation                | âœ… Completed |
| Phase 2 | Data cleaning, feature engineering, quality report | âœ… Completed |
| Phase 3 | Exploratory Data Analysis & insight generation     | âœ… Completed |
| Phase 4 | Modeling & evaluation                              | â³ Upcoming  |
| Phase 5 | Dashboard, final report & presentation             | â³ Upcoming  |

---

## ğŸ“Š Current Outputs

* `cleaned_master.csv` â†’ Unified master dataset (36 columns, ~36K rows)
* `eda_summary.json` / `eda_kpis.json` â†’ Key metrics, insights, and completeness summary
* `profiling_categorical_snapshot.csv` / `profiling_numeric_snapshot.csv` â†’ Profiling statistics
* `data_quality_report.json` â†’ Completeness & validity summary
* 35+ auto-saved visualizations in `/outputs/figures/` across 7 categories (univariate, temporal, salary, geo, skills, relationships, outliers)

---

## ğŸš€ How to Run

```bash
git clone https://github.com/<username>/datathon-2025.git
cd datathon-2025
pip install -r requirements.txt
jupyter notebook notebooks/03_eda.ipynb
```

---

## ğŸ“§ Contact

**Team Contact:** [work.tejasx@gmail.com](mailto:work.tejasx@gmail.com)

---
