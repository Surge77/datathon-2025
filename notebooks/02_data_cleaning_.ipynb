{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68efa021",
   "metadata": {},
   "source": [
    "## 1) Imports & paths\n",
    "##### ðŸ’¡ Concept\n",
    "Before manipulating data, we must set up the environment â€” define where data lives and where cleaned outputs will go. Think of this like laying out your lab tools before starting an experiment. Without consistent paths, reproducibility collapses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1ab50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re, json\n",
    "from pathlib import Path\n",
    "RNG = 42\n",
    "\n",
    "ROOT = Path.cwd().parents[0] if (Path.cwd().name == \"notebooks\") else Path.cwd()\n",
    "DATA_RAW = ROOT / \"data\" / \"raw\"\n",
    "DATA_PROC = ROOT / \"data\" / \"processed\"\n",
    "REPORTS = ROOT / \"reports\"\n",
    "FIGS = ROOT / \"outputs\" / \"figures\"\n",
    "\n",
    "DATA_PROC.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce652e",
   "metadata": {},
   "source": [
    "## 2) Load raw datasets\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "At its core, â€œloadingâ€ isnâ€™t just reading CSVs â€” itâ€™s about validating the data contract.\n",
    "Each dataset has a schema (columns, types, units). Before merging, you must check whether those schemas align or conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(DATA_RAW / \"dataset1_data_science_job.csv\")\n",
    "df2 = pd.read_csv(DATA_RAW / \"dataset2_all_job_post.csv\")\n",
    "df3 = pd.read_csv(DATA_RAW / \"dataset3_ai_job_dataset.csv\")\n",
    "\n",
    "for name, df in {\"df1\": df1, \"df2\": df2, \"df3\": df3}.items():\n",
    "    print(name, df.shape); display(df.head(2)); display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b99ea",
   "metadata": {},
   "source": [
    "## 3) Profiling snapshot (lightweight)\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "Profiling is the diagnostic stage of cleaning â€” like running blood tests before prescribing medicine.\n",
    "It tells you whatâ€™s wrong: missing values, strange datatypes, duplicates, etc.\n",
    "Without this, cleaning becomes random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(df: pd.DataFrame, name: str) -> dict:\n",
    "    \"\"\"Return basic profile stats for df. \n",
    "    Time: O(n * c). Space: O(c).\"\"\"\n",
    "    return {\n",
    "        \"rows\": len(df),\n",
    "        \"cols\": df.shape[1],\n",
    "        \"na_counts\": df.isna().sum().to_dict(),\n",
    "        \"dup_rows\": int(df.duplicated().sum()),\n",
    "        \"numeric_cols\": df.select_dtypes(include=\"number\").columns.tolist(),\n",
    "        \"object_cols\": df.select_dtypes(include=\"object\").columns.tolist(),\n",
    "    }\n",
    "\n",
    "profiles = {k: profile(v, k) for k, v in {\"df1\": df1, \"df2\": df2, \"df3\": df3}.items()}\n",
    "print(json.dumps(profiles, indent=2)[:2000], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900ec10",
   "metadata": {},
   "source": [
    "## 4ï¸) Schema Harmonization\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "Datasets from different sources often call the same thing by different names â€” e.g., job_title vs title.\n",
    "Before merging, we need a shared vocabulary.\n",
    "This is the lingua franca of your data â€” making sure everyone (and every dataset) â€œspeaks the same languageâ€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52015bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLMAP = {\n",
    "    \"title\": \"job_title\",\n",
    "    \"jobTitle\": \"job_title\",\n",
    "    \"category\": \"job_category\",          # NEW\n",
    "    \"skills\": \"required_skills\",\n",
    "    \"job_skill_set\": \"required_skills\",  # NEW\n",
    "    \"experience\": \"experience_level\",\n",
    "    \"exp_level\": \"experience_level\",\n",
    "    \"salary_in_usd\": \"salary_usd\",       # NEW\n",
    "    \"salary_usd\": \"salary_usd\",\n",
    "    \"salaryLocal\": \"salary_local\",       # just in case\n",
    "    \"salary\": \"salary\",\n",
    "    \"location\": \"company_location\",\n",
    "    \"posted_at\": \"posting_date\",\n",
    "}\n",
    "\n",
    "DATE_COLS = [\"posting_date\", \"application_deadline\"]\n",
    "NUM_COLS  = [\"salary_usd\", \"salary\", \"salary_local\",\n",
    "             \"remote_ratio\", \"years_experience\",\n",
    "             \"benefits_score\", \"job_description_length\"]\n",
    "\n",
    "def harmonize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize schema + dtypes. Time: O(n+c), Space: ~O(1) extra.\"\"\"\n",
    "    out = df.rename(columns={k:v for k,v in COLMAP.items() if k in df.columns}).copy()\n",
    "    for d in DATE_COLS:\n",
    "        if d in out.columns:\n",
    "            out[d] = pd.to_datetime(out[d], errors=\"coerce\")\n",
    "    for n in NUM_COLS:\n",
    "        if n in out.columns:\n",
    "            out[n] = pd.to_numeric(out[n], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "df1h, df2h, df3h = map(harmonize, (df1, df2, df3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749790df",
   "metadata": {},
   "source": [
    "## 5ï¸) Missing Values & â€œUnknownâ€ Categories\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "Missing data is information â€” it tells you where the system failed to observe.  \n",
    "We never randomly â€œfillâ€ it; we reason about why itâ€™s missing.  \n",
    "Duplicates distort truth â€” one job posted twice looks like double demand.  \n",
    "Here, imputation = an informed guess.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "241e7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df):\n",
    "    out = df.copy()\n",
    "    for c in out.select_dtypes(\"object\"):\n",
    "        out[c] = out[c].fillna(\"Unknown\")\n",
    "    for c in out.select_dtypes(\"number\"):\n",
    "        out[c] = out[c].fillna(out[c].median())\n",
    "    return out.drop_duplicates()\n",
    "\n",
    "df1c, df2c, df3c = map(fill_missing, (df1h, df2h, df3h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c41c3",
   "metadata": {},
   "source": [
    "## 6) De-dupe & Validity Checks (ID rules, logical ranges)\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "Cleaning is not just about fixing missing or noisy data â€” itâ€™s about protecting logical truth in your dataset.\n",
    "Before modeling or visualization, you must ensure that:\n",
    "\n",
    "No duplicate rows distort your results.\n",
    "\n",
    "Key fields (like job title, salary, or experience) obey reasonable rules.\n",
    "\n",
    "No impossible records exist (like â€œ-5 years experienceâ€ or â€œposted in 2030â€)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e382face",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dupes_ranges(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    De-dupe using strongest key available, then clamp obviously invalid ranges\n",
    "    and lightly winsorize salary-like columns.\n",
    "    Time: ~O(n). Space: ~O(n) due to copy/masks.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- 1) De-dupe ---\n",
    "    if \"job_id\" in out.columns:\n",
    "        out = out.drop_duplicates(subset=[\"job_id\"])\n",
    "    else:\n",
    "        cand_keys = [\"job_title\", \"company_name\", \"company_location\", \"posting_date\"]\n",
    "        subset = [c for c in cand_keys if c in out.columns]\n",
    "        # If nothing is available, fall back to full-row dedupe\n",
    "        out = out.drop_duplicates(subset=subset if subset else None)\n",
    "\n",
    "    # Optional: normalize posting_date to date for dedupe consistency upstream\n",
    "    # if \"posting_date\" in out.columns and np.issubdtype(out[\"posting_date\"].dtype, np.datetime64):\n",
    "    #     out[\"posting_date\"] = out[\"posting_date\"].dt.normalize()\n",
    "\n",
    "    # --- 2) Hard bounds (domain rules) ---\n",
    "    bounds = {\n",
    "        \"remote_ratio\": (0, 100),\n",
    "        \"years_experience\": (0, 50),\n",
    "        \"job_description_length\": (0, None),  # min 0\n",
    "        \"benefits_score\": (0, 100),           # adjust later if your canonical scale is 0..1\n",
    "    }\n",
    "    for col, (lo, hi) in bounds.items():\n",
    "        if col in out.columns:\n",
    "            # coerce numerics defensively\n",
    "            out[col] = pd.to_numeric(out[col], errors=\"coerce\")\n",
    "            if lo is not None:\n",
    "                out[col] = out[col].clip(lower=lo)\n",
    "            if hi is not None:\n",
    "                out[col] = out[col].clip(upper=hi)\n",
    "\n",
    "    # --- 3) Light winsorization for salaries ---\n",
    "    for s in (\"salary_usd\", \"salary\", \"salary_local\"):\n",
    "        if s in out.columns:\n",
    "            out[s] = pd.to_numeric(out[s], errors=\"coerce\")\n",
    "            ser = out[s].dropna()\n",
    "            if ser.size >= 10:  # need enough data to estimate tails\n",
    "                q = ser.quantile([0.005, 0.995])\n",
    "                low, high = float(q.iloc[0]), float(q.iloc[1])\n",
    "                # If quantiles are nan (pathological), skip safely\n",
    "                if np.isfinite(low) and np.isfinite(high) and low <= high:\n",
    "                    out[s] = out[s].clip(lower=low, upper=high)\n",
    "            # Enforce strictly positive salaries (scraper glitches)\n",
    "            out[s] = out[s].clip(lower=1)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d0d07",
   "metadata": {},
   "source": [
    "## 7) Outlier handling (salary via IQR within job_titleÃ—location)\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "\n",
    "Outliers arenâ€™t â€œwrong,â€ theyâ€™re rare. We cap them only when they distort aggregates/models. Do it within comparable groups (e.g., job_title Ã— company_location) so a Bay Area Staff ML salary doesnâ€™t clamp a Pune junior role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793c342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_salary_iqr(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cap salary_usd outliers per (job_title Ã— company_location) using IQR.\n",
    "    Guards: dtype coercion, min group size, two-tailed fences, NaN safety.\n",
    "    Time: ~O(n). Space: ~O(n).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    out = df.copy()\n",
    "\n",
    "    if \"salary_usd\" not in out.columns:\n",
    "        return out\n",
    "\n",
    "    # ensure numeric\n",
    "    out[\"salary_usd\"] = pd.to_numeric(out[\"salary_usd\"], errors=\"coerce\")\n",
    "\n",
    "    grp_keys = [k for k in [\"job_title\", \"company_location\"] if k in out.columns]\n",
    "    if not grp_keys:\n",
    "        # No grouping keys â†’ optional global cap (or just return)\n",
    "        return out\n",
    "\n",
    "    # optional: drop rows with all-NaN salary before stats\n",
    "    valid = out.dropna(subset=[\"salary_usd\"])\n",
    "\n",
    "    # min group size mask\n",
    "    sizes = valid.groupby(grp_keys)[\"salary_usd\"].size()\n",
    "    big_groups = sizes[sizes >= 8]  # tweak threshold as needed\n",
    "\n",
    "    if big_groups.empty:\n",
    "        return out\n",
    "\n",
    "    big_idx = big_groups.index  # MultiIndex of groups\n",
    "\n",
    "    # compute fences on big groups only\n",
    "    q1 = valid[valid.set_index(grp_keys).index.isin(big_idx)] \\\n",
    "             .groupby(grp_keys)[\"salary_usd\"].quantile(0.25)\n",
    "    q3 = valid[valid.set_index(grp_keys).index.isin(big_idx)] \\\n",
    "             .groupby(grp_keys)[\"salary_usd\"].quantile(0.75)\n",
    "\n",
    "    iqr = (q3 - q1)\n",
    "    lo = (q1 - 1.5 * iqr).rename(\"lo\")\n",
    "    hi = (q3 + 1.5 * iqr).rename(\"hi\")\n",
    "\n",
    "    # join fences back by group keys (handles MultiIndex)\n",
    "    fences = pd.concat([lo, hi], axis=1)\n",
    "    out = out.join(fences, on=grp_keys)\n",
    "\n",
    "    # cap where fences exist; keep original where fences are NaN (small groups, NaNs)\n",
    "    has_lo = out[\"lo\"].notna()\n",
    "    has_hi = out[\"hi\"].notna()\n",
    "    out.loc[has_lo, \"salary_usd\"] = np.maximum(out.loc[has_lo, \"salary_usd\"], out.loc[has_lo, \"lo\"])\n",
    "    out.loc[has_hi, \"salary_usd\"] = np.minimum(out.loc[has_hi, \"salary_usd\"], out.loc[has_hi, \"hi\"])\n",
    "\n",
    "    # final sanity: enforce strictly positive salaries\n",
    "    out[\"salary_usd\"] = out[\"salary_usd\"].clip(lower=1)\n",
    "\n",
    "    return out.drop(columns=[c for c in [\"lo\", \"hi\"] if c in out.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4c005",
   "metadata": {},
   "source": [
    "## 8) Skill standardization (lowercase, split, synonyms)\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "\n",
    "Skills are messy â€” â€œPythonâ€, â€œpython3â€, â€œPYTHONâ€ â€” but all mean the same ability.\n",
    "We normalize by lowercasing, removing punctuation, and mapping synonyms.\n",
    "This step ensures that your model doesnâ€™t think â€œJSâ€ and â€œJavaScriptâ€ are different universes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff503f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Skill Normalization â€” CSV-aware & Robust\n",
    "# Time:  O(n * k)  |  Space: O(n * k)\n",
    "# n = rows, k = avg skills per row\n",
    "# ============================================\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Helper: pick the right raw skill column (dataset2 uses 'job_skill_set')\n",
    "def _get_skills_series(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"required_skills\" in df.columns:\n",
    "        return df[\"required_skills\"]\n",
    "    if \"job_skill_set\" in df.columns:\n",
    "        # normalize name so downstream is consistent\n",
    "        df = df.copy()\n",
    "        df[\"required_skills\"] = df[\"job_skill_set\"]\n",
    "        return df[\"required_skills\"]\n",
    "    # fallback: empty strings\n",
    "    return pd.Series([\"\"] * len(df), index=df.index)\n",
    "\n",
    "# 1) Synonym map â€” extend as you discover variants in your EDA\n",
    "SYN = {\n",
    "    # Python family\n",
    "    \"python3\": \"python\", \"py\": \"python\",\n",
    "    # JavaScript family\n",
    "    \"js\": \"javascript\", \"ecmascript\": \"javascript\",\n",
    "    \"node.js\": \"nodejs\", \"node js\": \"nodejs\", \"node\": \"nodejs\",\n",
    "    \"ts\": \"typescript\",\n",
    "    # C-family\n",
    "    \"c#\": \"csharp\", \"c-sharp\": \"csharp\", \"c sharp\": \"csharp\",\n",
    "    \"cpp\": \"c++\", \"c plus plus\": \"c++\",\n",
    "    # Data/ML libs\n",
    "    \"sklearn\": \"scikit-learn\", \"scikit learn\": \"scikit-learn\",\n",
    "    \"tf\": \"tensorflow\", \"tf2\": \"tensorflow\",\n",
    "    # DBs\n",
    "    \"postgresql\": \"postgres\", \"psql\": \"postgres\",\n",
    "    # FE frameworks\n",
    "    \"react.js\": \"react\", \"reactjs\": \"react\",\n",
    "    \"vue.js\": \"vue\", \"vuejs\": \"vue\",\n",
    "    # Clouds\n",
    "    \"amazon web services\": \"aws\", \"aws cloud\": \"aws\",\n",
    "    \"ms azure\": \"azure\", \"gcp\": \"google cloud\",\n",
    "}\n",
    "\n",
    "# 2) Tiny normalizer helpers\n",
    "_version_pat = re.compile(r\"\\s*\\d+([._-]\\d+)*$\")     # strips trailing versions: \"python 3.11\" -> \"python\"\n",
    "_delim_pat   = re.compile(r\"[|,/\\s]+\")\n",
    "\n",
    "def _canon(token: str) -> str:\n",
    "    t = token.strip().lower()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    # unify some punctuation variants\n",
    "    t = t.replace(\"â€“\", \"-\").replace(\"â€”\", \"-\").replace(\"_\", \" \")\n",
    "    # drop trailing version numbers\n",
    "    t = _version_pat.sub(\"\", t).strip()\n",
    "    # apply dictionary\n",
    "    return SYN.get(t, t)\n",
    "\n",
    "def normalize_skills_cell(cell) -> list[str]:\n",
    "    \"\"\"Return sorted unique canonical skills from one cell.\"\"\"\n",
    "    if pd.isna(cell) or not str(cell).strip():\n",
    "        return []\n",
    "    tokens = _delim_pat.split(str(cell).lower())\n",
    "    cleaned = [_canon(tok) for tok in tokens if tok and tok.strip()]\n",
    "    # drop empties, uniquify, sort for reproducibility\n",
    "    return sorted({t for t in cleaned if t})\n",
    "\n",
    "# 3) Apply to each of your dataframes (df1c, df2c, df3c)\n",
    "for df in (df1c, df2c, df3c):\n",
    "    skills_series = _get_skills_series(df)\n",
    "    df[\"required_skills\"] = skills_series  # ensures the column exists uniformly\n",
    "    df[\"required_skills_norm\"] = skills_series.apply(normalize_skills_cell)\n",
    "    # Optional: BI-friendly string column (pipe-joined)\n",
    "    df[\"required_skills_norm_str\"] = df[\"required_skills_norm\"].apply(lambda xs: \"|\".join(xs))\n",
    "\n",
    "# 4) (Optional) Quick sanity peek â€” comment out in final notebook\n",
    "def _preview(df, name):\n",
    "    if {\"required_skills\",\"required_skills_norm\"}.issubset(df.columns):\n",
    "        sample = df.loc[df[\"required_skills\"].notna(), [\"required_skills\",\"required_skills_norm\"]].head(5)\n",
    "        print(f\"\\n[{name}] skill normalization preview:\")\n",
    "        for _, row in sample.iterrows():\n",
    "            print(\"  raw ->\", row[\"required_skills\"])\n",
    "            print(\"  norm->\", row[\"required_skills_norm\"])\n",
    "\n",
    "_preview(df1c, \"df1c\")\n",
    "_preview(df2c, \"df2c\")\n",
    "_preview(df3c, \"df3c\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7caab6",
   "metadata": {},
   "source": [
    "## 9) Minimal Feature Engineering (salary_midpoint, posting_year, skill_count)\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "\n",
    "Feature engineering converts raw facts into signals.\n",
    "By extracting date parts or skill counts, we help models see structure in what was previously unstructured.\n",
    "For instance, knowing â€œposting_monthâ€ helps detect hiring seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5b54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _len_safe(x):\n",
    "    return len(x) if isinstance(x, (list, tuple)) else 0\n",
    "\n",
    "def add_basic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1) Salary midpoint (coerce if needed)\n",
    "    if {\"salary_min\", \"salary_max\"}.issubset(out.columns):\n",
    "        out[\"salary_min\"] = pd.to_numeric(out[\"salary_min\"], errors=\"coerce\")\n",
    "        out[\"salary_max\"] = pd.to_numeric(out[\"salary_max\"], errors=\"coerce\")\n",
    "        out[\"salary_midpoint\"] = (out[\"salary_min\"] + out[\"salary_max\"]) / 2\n",
    "    elif \"salary_usd\" in out.columns:\n",
    "        out[\"salary_midpoint\"] = pd.to_numeric(out[\"salary_usd\"], errors=\"coerce\")\n",
    "    elif \"salary\" in out.columns:\n",
    "        out[\"salary_midpoint\"] = pd.to_numeric(out[\"salary\"], errors=\"coerce\")\n",
    "\n",
    "    # 2) Posting date breakdown (ensure datetime first)\n",
    "    if \"posting_date\" in out.columns:\n",
    "        if not np.issubdtype(out[\"posting_date\"].dtype, np.datetime64):\n",
    "            out[\"posting_date\"] = pd.to_datetime(out[\"posting_date\"], errors=\"coerce\")\n",
    "        out[\"posting_year\"] = out[\"posting_date\"].dt.year\n",
    "        out[\"posting_month\"] = out[\"posting_date\"].dt.month\n",
    "        # optional:\n",
    "        # out[\"posting_quarter\"] = out[\"posting_date\"].dt.quarter\n",
    "        # out[\"is_recent\"] = out[\"posting_date\"] >= (pd.Timestamp.now() - pd.Timedelta(days=90))\n",
    "\n",
    "    # 3) Skill count (robust to non-lists)\n",
    "    if \"required_skills_norm\" in out.columns:\n",
    "        out[\"skill_count\"] = out[\"required_skills_norm\"].apply(_len_safe)\n",
    "    else:\n",
    "        out[\"skill_count\"] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "# Apply to all three cleaned dataframes\n",
    "df1f, df2f, df3f = map(add_basic_features, (df1c, df2c, df3c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14608a10",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['posting_year', 'posting_month'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf1f\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msalary_midpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposting_year\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposting_month\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskill_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\tdmne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tdmne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tdmne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['posting_year', 'posting_month'] not in index\""
     ]
    }
   ],
   "source": [
    "print(df1f[[\"salary_midpoint\",\"posting_year\",\"posting_month\",\"skill_count\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c51007",
   "metadata": {},
   "source": [
    "## 10) Data-Quality Metrics (completeness, duplicates, validity)\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "\n",
    "Once the dataset looks â€œclean,â€ we must prove it.\n",
    "Data quality is the empirical backbone of credibility. We quantify how trustworthy our cleaned data is instead of assuming itâ€™s fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05bc9118",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'master' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dq\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# âœ… CALL IT HERE\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m dq \u001b[38;5;241m=\u001b[39m write_data_quality_report(\u001b[43mmaster\u001b[49m, REPORTS)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'master' is not defined"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ðŸ”Ž STEP 10 â€” Data Quality Metrics (Robust)\n",
    "# Time: O(nÂ·c) | Space: O(c)\n",
    "# ===============================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure these exist\n",
    "# master   â†’ your merged DataFrame (from Step 9)\n",
    "# REPORTS  â†’ Path to your reports folder (created in Step 1)\n",
    "\n",
    "def write_data_quality_report(master: pd.DataFrame, reports_dir: Path) -> dict:\n",
    "    \"\"\"Generate and save a data-quality summary for the cleaned dataset.\"\"\"\n",
    "    reports_dir = Path(reports_dir)\n",
    "    dq = {\n",
    "        \"rows\": int(len(master)),\n",
    "        \"cols\": int(master.shape[1]),\n",
    "    }\n",
    "\n",
    "    # --- Duplicates ---\n",
    "    dq[\"duplicates\"] = int(master.duplicated().sum())\n",
    "\n",
    "    # --- Completeness (% non-null per column) ---\n",
    "    completeness = {}\n",
    "    list_nonempty = {}\n",
    "    for c in master.columns:\n",
    "        comp = float(master[c].notna().mean()) if len(master) else 0.0\n",
    "        completeness[c] = round(comp, 4)\n",
    "        # list-like columns (e.g., required_skills_norm)\n",
    "        if master[c].dtype == \"object\" and master[c].apply(lambda x: isinstance(x, (list, tuple))).any():\n",
    "            nonempty = float(master[c].apply(lambda x: isinstance(x, (list, tuple)) and len(x) > 0).mean())\n",
    "            list_nonempty[c] = round(nonempty, 4)\n",
    "\n",
    "    dq[\"completeness\"] = completeness\n",
    "    if list_nonempty:\n",
    "        dq[\"list_nonempty_ratio\"] = list_nonempty\n",
    "\n",
    "    # --- Validity metrics ---\n",
    "    validity = {}\n",
    "\n",
    "    # Date sanity\n",
    "    if \"posting_date\" in master.columns:\n",
    "        pd_coerced = pd.to_datetime(master[\"posting_date\"], errors=\"coerce\")\n",
    "        in_range = (pd_coerced >= pd.Timestamp(\"2010-01-01\")) & (pd_coerced <= pd.Timestamp(\"2025-12-31\"))\n",
    "        validity[\"posting_date_in_range\"] = float(in_range.mean())\n",
    "\n",
    "    # Salary positivity\n",
    "    if \"salary_usd\" in master.columns:\n",
    "        sal = pd.to_numeric(master[\"salary_usd\"], errors=\"coerce\")\n",
    "        validity[\"salary_usd_positive\"] = float((sal > 0).mean())\n",
    "\n",
    "    # Remote ratio within 0â€“100\n",
    "    if \"remote_ratio\" in master.columns:\n",
    "        rr = pd.to_numeric(master[\"remote_ratio\"], errors=\"coerce\")\n",
    "        validity[\"remote_ratio_0_100\"] = float(((rr >= 0) & (rr <= 100)).mean())\n",
    "\n",
    "    # Years experience 0â€“50\n",
    "    if \"years_experience\" in master.columns:\n",
    "        ye = pd.to_numeric(master[\"years_experience\"], errors=\"coerce\")\n",
    "        validity[\"years_experience_0_50\"] = float(((ye >= 0) & (ye <= 50)).mean())\n",
    "\n",
    "    dq[\"validity\"] = validity\n",
    "\n",
    "    # --- Numeric quick stats (min, max, missing%) ---\n",
    "    numeric_ranges = {}\n",
    "    for c in master.select_dtypes(include=[np.number]).columns:\n",
    "        s = master[c]\n",
    "        if s.notna().any():\n",
    "            numeric_ranges[c] = {\n",
    "                \"min\": float(np.nanmin(s)),\n",
    "                \"max\": float(np.nanmax(s)),\n",
    "                \"pct_missing\": round(float(s.isna().mean()), 4)\n",
    "            }\n",
    "        else:\n",
    "            numeric_ranges[c] = {\"min\": None, \"max\": None, \"pct_missing\": 1.0}\n",
    "    dq[\"numeric_ranges\"] = numeric_ranges\n",
    "\n",
    "    # --- Save JSON ---\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = reports_dir / \"data_quality_report.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(dq, f, indent=2)\n",
    "\n",
    "    print(f\"âœ… Data-Quality Report written â†’ {out_path}\")\n",
    "    return dq\n",
    "\n",
    "# âœ… CALL IT HERE\n",
    "dq = write_data_quality_report(master, REPORTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b18ce",
   "metadata": {},
   "source": [
    "## 11) Save Artifacts & Summary (What Changed Â· Counts Before/After)\n",
    "\n",
    "##### ðŸ’¡ Concept\n",
    "\n",
    "Science isnâ€™t just resultsâ€”itâ€™s reproducible results.\n",
    "Saving artifacts ensures any other analyst can rebuild your exact cleaned dataset tomorrow with zero guesswork.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
