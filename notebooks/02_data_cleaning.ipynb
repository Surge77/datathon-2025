{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68efa021",
   "metadata": {},
   "source": [
    "## 1) Imports & paths\n",
    "##### Before manipulating data, we must set up the environment — define where data lives and where cleaned outputs will go. Think of this like laying out your lab tools before starting an experiment. Without consistent paths, reproducibility collapses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1ab50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re, json\n",
    "from pathlib import Path\n",
    "RNG = 42\n",
    "\n",
    "ROOT = Path.cwd().parents[0] if (Path.cwd().name == \"notebooks\") else Path.cwd()\n",
    "DATA_RAW = ROOT / \"data\" / \"raw\"\n",
    "DATA_PROC = ROOT / \"data\" / \"processed\"\n",
    "REPORTS = ROOT / \"reports\"\n",
    "FIGS = ROOT / \"outputs\" / \"figures\"\n",
    "\n",
    "DATA_PROC.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce652e",
   "metadata": {},
   "source": [
    "## 2) Load raw datasets\n",
    "\n",
    "##### At its core, “loading” isn’t just reading CSVs — it’s about validating the data contract.\n",
    "Each dataset has a schema (columns, types, units). Before merging, you must check whether those schemas align or conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(DATA_RAW / \"dataset1_data_science_job.csv\")\n",
    "df2 = pd.read_csv(DATA_RAW / \"dataset2_all_job_post.csv\")\n",
    "df3 = pd.read_csv(DATA_RAW / \"dataset3_ai_job_dataset.csv\")\n",
    "\n",
    "for name, df in {\"df1\": df1, \"df2\": df2, \"df3\": df3}.items():\n",
    "    print(name, df.shape); display(df.head(2)); display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b99ea",
   "metadata": {},
   "source": [
    "## 3) Profiling snapshot (lightweight)\n",
    "\n",
    "##### Profiling is the diagnostic stage of cleaning — like running blood tests before prescribing medicine.\n",
    "It tells you what’s wrong: missing values, strange datatypes, duplicates, etc.\n",
    "Without this, cleaning becomes random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(df: pd.DataFrame, name: str) -> dict:\n",
    "    \"\"\"Return basic profile stats for df. \n",
    "    Time: O(n * c). Space: O(c).\"\"\"\n",
    "    return {\n",
    "        \"rows\": len(df),\n",
    "        \"cols\": df.shape[1],\n",
    "        \"na_counts\": df.isna().sum().to_dict(),\n",
    "        \"dup_rows\": int(df.duplicated().sum()),\n",
    "        \"numeric_cols\": df.select_dtypes(include=\"number\").columns.tolist(),\n",
    "        \"object_cols\": df.select_dtypes(include=\"object\").columns.tolist(),\n",
    "    }\n",
    "\n",
    "profiles = {k: profile(v, k) for k, v in {\"df1\": df1, \"df2\": df2, \"df3\": df3}.items()}\n",
    "print(json.dumps(profiles, indent=2)[:2000], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
